# <a href="https://arxiv.org/abs/2601.18753" style="color: black !important;"> HALLUGUARD: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs [ICLR 2026] </a>

**Xinyue ZengÂ¹\*, Junhong LinÂ²\*, Yujun YanÂ³, Feng GuoÂ¹, Liang ShiÂ¹, Jun Wuâ´, Dawei ZhouÂ¹**  
*\* Equal contribution*

Â¹ Virginia Tech Â· Â² MIT Â· Â³ Dartmouth Â· â´ Michigan State University

---

## ğŸ“Œ Abstract

Large Language Models (LLMs) increasingly operate in high-stakes domains, yet their deployment is constrained by **hallucinations**â€”outputs that are fluent but incorrect or logically inconsistent. Existing hallucination detection methods typically focus on either factual errors or reasoning instability, rely on task-specific heuristics, or require external references, limiting robustness and scalability.

We introduce **HALLUGUARD**, a theory-grounded, NTK-based hallucination detection framework built upon a novel **Hallucination Risk Bound** that formally decomposes hallucinations into **data-driven** and **reasoning-driven** components. This unified view explains how hallucinations emerge and evolve during generation, and enables a single, architecture-agnostic score that detects both failure modes without supervision or external knowledge.

Across 10 benchmarks, 11 baselines, and 9 LLM backbones, HALLUGUARD consistently achieves state-of-the-art hallucination detection, and further improves test-time reasoning accuracy when used for score-guided inference.

---

## ğŸ§  Core Insight: Hallucinations Are Not One Failure Mode

We show that hallucinations arise from two fundamentally different mechanisms:

| Source | What breaks | When it breaks |
|--------|-------------|----------------|
| **Data-driven hallucinations** | Learned representations misalign with task semantics | Training / fine-tuning |
| **Reasoning-driven hallucinations** | Autoregressive decoding amplifies instability | Inference / multi-step generation |

Crucially, these errors interact and compound over decoding steps, which explains why existing detectors fail under long-horizon reasoning.

---

## ğŸ” Hallucination Risk Bound (Theory)

We formalize hallucination as deviation in a semantic hypothesis space and prove a **Hallucination Risk Bound**:

\[
\|u^* - \hat{u}\| \leq \underbrace{\|u^* - \mathbb{E}[\hat{u}]\|}_{\text{data-driven}} + \underbrace{\|\hat{u} - \mathbb{E}[\hat{u}]\|}_{\text{reasoning-driven}}
\]

**Interpretation (No fluff):**

- **Data-driven term** â†’ bounded by NTK spectrum geometry  
- **Reasoning-driven term** â†’ bounded by Jacobian amplification across decoding steps  
- Hallucinations start as representation gaps and escalate via unstable rollouts  

This is the first unified theoretical decomposition of hallucination emergence and evolution.

---

## ğŸ§© HALLUGUARD Score

To operationalize the bound, we derive a tractable, inference-time score:

\[
\text{HALLUGUARD}(\hat{u}) = \underbrace{\det(K)}_{\text{representation adequacy}} + \underbrace{\log \sigma_{\max}}_{\text{reasoning amplification}} - \underbrace{\log \kappa(K)^2}_{\text{spectral instability penalty}}
\]

where \(K\) is the NTK Gram matrix (over generated outputs), \(\sigma_{\max}\) is the maximum per-step hidden-state Jacobian spectral norm over the rollout, and \(\kappa(K)\) is the condition number of \(K\).

### What each term actually does

| Term | Captures | Why it matters |
|------|----------|-----------------|
| \(\det(K)\) | Semantic coverage | Flags factual / data gaps |
| \(\log \sigma_{\max}\) | Step-wise Jacobian growth | Flags reasoning drift |
| \(2\log \kappa(K)\) (penalty) | NTK conditioning | Penalizes brittle representations |

- âœ” No labels  
- âœ” No task heuristics  
- âœ” No external retrieval  
- âœ” Zero inference overhead  

### Technical specification (code verification)

The score must be assembled exactly as (per paper):

```text
HALLUGUARD = det(K) + log(Ïƒ_max) âˆ’ log(Îº(K)^2)
```

- **K**: NTK Gram from Jacobians of log-probabilities of generated tokens w.r.t. a fixed parameter subset; build \(G\) with rows \(g_t = \nabla_\theta f_t / \|g_t\|\) (one per decoding step), then \(K = G G^\top\). Eigenvalues of \(K\) are clamped with a small \(\varepsilon\) before taking \(\det(K)\) via \(\exp(\sum \log \lambda_i)\).
- **Ïƒ_max**: Maximum over steps \(t\) of the spectral norm of the hidden-state Jacobian \(J_t = \partial h_t / \partial h_{t-1}\) (or an allowed Lipschitz proxy). Use \(\max_t\), not an average.
- **Îº(K)**: \(\lambda_{\max}(K) / (\lambda_{\min}(K) + \varepsilon)\); the penalty term is \(-2\log \kappa(K)\).

Model must be frozen (no fine-tuning); generation must be stochastic (sampling or beam), not greedy.

**Code vs. spec:** Default evaluation scripts now call the true implementation (`halluguard_true.py`) for NTKâ€‘S3/HALLUGUARD. Proxy implementations remain in legacy helper code (e.g., `func/metric.py`) but are not used by default.

---

## ğŸ“Š Empirical Results

### Benchmarks

- **Data-grounded QA:** RAGTruth, SQuAD, NQ-Open, HotpotQA  
- **Reasoning:** GSM8K, MATH-500, BBH  
- **Instruction following:** TruthfulQA, Natural, HaluEval  

### Key outcomes

- State-of-the-art **AUROC / AUPRC** across all task families  
- Largest gains on reasoning benchmarks, where existing detectors collapse  
- Stronger improvements on smaller models, where hallucination risk is highest  
- Consistent cross-scale behavior from GPT-2 â†’ Llama-70B  

### Test-time inference (real payoff)

Using HALLUGUARD to guide beam search:

- **+10%** accuracy on MATH-500  
- **+15%** accuracy on Natural  

â†’ hallucination detection becomes **reasoning control**, not just scoring.

---

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ Hallucination/           # Main evaluation and pipelines
â”‚   â”œâ”€â”€ halluguard_true.py   # Paper HALLUGUARD score: det(K) + log Ïƒ_max âˆ’ log ÎºÂ²
â”‚   â”œâ”€â”€ pipeline/            # Generation (generate.py, generate_simple.py, generate_minimal.py)
â”‚   â”œâ”€â”€ func/                # Metrics, evalFunc, plot
â”‚   â”œâ”€â”€ dataeval/            # Data loaders (SQuAD, TruthfulQA, CoQA, NQ-Open, TriviaQA, â€¦)
â”‚   â”œâ”€â”€ models/              # Model loading, NLI, OpenAI helpers
â”‚   â”œâ”€â”€ utils/               # Parallel and other utilities
â”‚   â”œâ”€â”€ data/                # Outputs and run logs
â”‚   â”œâ”€â”€ metrics_output/      # Benchmark CSV results (SQuAD, GSM8K, MATH-500, â€¦)
â”‚   â”œâ”€â”€ intrinsic_probe/     # Intrinsic probing (correctness, resampling)
â”‚   â”œâ”€â”€ gpu_evaluation_all.py
â”‚   â”œâ”€â”€ gpu_evaluation_llm.py
â”‚   â”œâ”€â”€ evaluation.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ Beam Search/             # Score-guided beam search and reward models
â”‚   â”œâ”€â”€ reward_model/        # NTK reward uses halluguard_true (paper formula)
â”‚   â””â”€â”€ run/                 # run_score, run_train
â””â”€â”€ README.md
```

**Implementation note:** All evaluation scripts and pipelines use the paper HALLUGUARD implementation (`halluguard_true.py`): NTK Gram from per-step parameter gradients, Lipschitz proxy for Ïƒ_max, and score det(K) + log Ïƒ_max âˆ’ log ÎºÂ².

---

## ğŸš€ Quick Start

### Install

```bash
cd Hallucination
pip install -r requirements.txt
```

### Run pipeline (paper HALLUGUARD score)

From the repo root or from `Hallucination/`:

```bash
./Hallucination/run_pipeline.sh --model gpt2 --dataset coqa --device cuda --num_generations_per_prompt 2 --fraction_of_data_to_use 0.01
```

Or from `Hallucination/` with `PYTHONPATH=. python pipeline/generate_simple.py ...`. The pipeline computes the paper score (det(K) + log Ïƒ_max âˆ’ log ÎºÂ²) via `halluguard_true.py`.

### Evaluation scripts

- **GPU benchmarks:** `Hallucination/gpu_evaluation_all.py`, `Hallucination/gpu_evaluation_llm.py` â€” NTK-S3 uses the true HALLUGUARD score.
- **Evaluation:** `Hallucination/evaluation.py` â€” same paper-aligned score.

Put dataset files under `Hallucination/data/datasets/` (e.g. CoQA: `coqa-dev-v1.0.json`). See `Hallucination/README_PIPELINE.md` for options.

### Score-guided inference (beam search)

From `Beam Search/`, use the NTK reward model (paper formula) in beam search: `reward_model/ntk_reward.py`, `run/run_score.py`. The reward model imports `halluguard_true` from the repoâ€™s `Hallucination` folder so the repo root must be the parent of both `Hallucination/` and `Beam Search/`.

---

## ğŸ§  Why This Matters (Positioning)

HALLUGUARD is not another heuristic detector.  
It is a **theoretically grounded reliability primitive**:

- **Explains** why hallucinations happen  
- **Detects** how they evolve  
- **Controls** where reasoning goes wrong  

This makes it directly extensible to:

- Multi-turn dialogue safety  
- Agentic planning  
- Test-time compute allocation  
- Early-warning hallucination prevention  

---

## ğŸ“– Citation

```bibtex
@misc{zeng2026halluguarddemystifyingdatadrivenreasoningdriven,
      title={HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs}, 
      author={Xinyue Zeng and Junhong Lin and Yujun Yan and Feng Guo and Liang Shi and Jun Wu and Dawei Zhou},
      year={2026},
      eprint={2601.18753},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2601.18753}, 
}
```
